== Compiler

Components
==========

Queries
-------

The query compiler and optimizer is going to be a crucial element in 
optimizing tree traversals based on a set of queries and depdenencies.

We've seen that a query can be summed up as:

- a base node (or node set), from which the query is run
- an axis to traverse, with an optional depth limit
- a selector on the node name, either direct or with an expression
- a set predicate containing sub-expressions, which in most cases
  require dynamic evaluation.

Queries can be chained to form selectors, and queries can be nested
within the predicate sub-expression to perform parallel sub-queries.

In its most basic form, given a set of queries, the compiler should be
able to *order them* so that they can be applied in the least possible
amout of traversals.

The other thing that the compiler should do is to *assign queries a priority*
based on their specificity. In case of pattern matching, the priority will be
useful to known which query should apply first.

Aditionnally, the query compiler should be able to *identify if a query
is a subset of another* and derive a *query that is the intersection
of a set of other queries*.


Traversals
----------

As TLang is intented to support *incremental updates*, an important part of
the compiler is to define and schedule traversals to propagate (eager)
or compute (lazy) the effects resulting of an update on the tree. 

There should be as little computation required at runtime to determine these
traversals, which means that they either need to be hardcoded or implemented
as part of a state machine.

In any case, the *traversal scheduler* should take a set of events (node 
added, removed, updated) and schedule the applications of their effects, 
either eagerly (direct propagation) or lazily (deferred computation), based
on hints in TLang or some criteria identified by the compiler.

The resulting traversal state machine should basically work like this:

1) A set of changes is given
2) Each change results in a sequence/tree of queries to be scheduled
3) Queries are scheduled in traversals (top-down,bottom-up,left-right,right-left)
   and cycles (some synthetic attributes might require more than one cycle to
   compute the result)
4) Query result should be 


Errors
======

- Transducer/schema: attribute not defined 
- Transducer/schema: node not defined
- Transducer/effect: incompatible attribute type
- Transducer/effect: incompatible node type
- Transducer/effect: mutating synthetic attribute
- Transducer/effect: recursive attribute definition

- Transducer/process: function not defined
- Transducer/process: missing argument
- Transducer/process: incompatible argument
- Transducer/process: incompatible return type
- Transducer/process: undefined variable


Warnings
========

- Transducer/effect: cylcic effect (A→B, B→C)
- Transducer/effect: cascading changes (triggers more than one top-down, bottom-up)

Formatting
==========

For any error or warning, there should be:

- one or more source ranges indicating where the error happened
  and the possible other declarations involved.

- a suggestion of the next action to resolve the warning/error.

```
XXX not defined, did you mean 'YYY', 'ZZZ' or 'WWW'?
```

Compiler phases
===============

- Parse the schema

  - Ensure that all the reference nodes are defined
  - Warns when nodes are declared but not reachable

- Parse the transducers

  - Ensure that each node and attribute is declared in the schema
  - Ensure that every function is defined in the interface
  - Warns when expressions are potentially sub-optimal and asks for an 
    annotation to confirm it's OK.

- Compute the mutation state machine

  - Based on all the attributes and their relations, compute a single
    mutation state machine taking `(node,transition,stack)` that can then
    be used by every attribute to trigger the matches.

  - Traversals should be minimized using some strategy that has to be defined

- Implementation generation

  - The data structures should be synthesized based on different strategies
    than are then evaluated based on the theory (what the compiler thinks
    is the best) and in practice (by feeding many synthesized test cases)


Required tools:

- Given a schema, produce a sample tree of arbitrary complexity, to be used
  in the identification of the best data structures.


Optimizations
=============

Queries
-------

Queries should be grouped by similarity, and if many rules/effects involve
the same query, then the queries should be cached. If the queries have
dynamic predicates, indexes should be created/suggested in order to
speed up everything.

Query/Traversal Optimization
============================

Looking at the following example:

```
(attr
	({expr-*:NODE}  @meta-level)
	(add
		(max     ./expr-*/@meta-level)
		(int (first ./expr-value-template))))
```

We have the following dependency graph


```
;; If the meta-level of an expr-* node changes
;; the parent @meta-level changes as well.
expr-*/meta-level   → .\expr-*/@meta-level

;; The the expr-template changes (ie. it is added
;; or removed),then the meta-level is invalidated.
expr-value-template → .\expr-*/@meta-level
```

now, the computation looks like this:

```
(add  (max A:List[Attribute]) (int (first B:List[Node])))
```

If we don't want to have to re-do the full query and
recompute the values completely, we'd need to listen 
for changes and try an incremental update:

- For `max`, we would need to know the current value. We
  only need to recompute if a removed value is the 
  maximum. If a new value is added, we can incrementally
  update it.

- For `first`, we will need to always recompute. This should
  not be a problems are selections should be lazy.

This means that incremental computations can work only
if they store their previous value, which implies some
kind of memoization. This memoization needs to be very
lightweight as otherwise it's going to cost memory
and CPU and offset the benefits.

In terms of data storage, this means that each computation
would produce `(VALUE,CONTEXT)` where the value 
is the computed attirbute/node and context is the 
memoization of the parts of the computation that might change 
(the selections).

The selections can then be used to derive a dependency
graph and know which attributes should be invalidated.

Now this leads to the two main strategies for updating the data:

- lazy, which means that any change would set the dependent node/attribute as dirty. The value will be updated/recalculated next time it is accessed, but there is no opportunity for incremental computation.

- eager, which means that the change is propagated directly. This has a high cost if the synthetic values do not support incremental computation, and might also trigger infinite/feedback loops. The advantage, however, is that if the computation time is large (ie. many nodes to process, or a complex traversal), eager is going to have the upper hand.




# EOF - vim: ts=2 sw=2 et syn=texto
